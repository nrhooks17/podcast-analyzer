version: '3.8'

services:
  # PostgreSQL Database
  postgres:
    image: postgres:15
    container_name: podcast_postgres
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-podcast_analyzer}
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-postgres}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5433:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-postgres}"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Zookeeper (required for Kafka)
  # NOTE: This is not secured for production use
  # In production, proper SASL/SSL authentication would be required
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: podcast_zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zk_data:/var/lib/zookeeper/data

  # Apache Kafka
  # NOTE: This is not secured for production use
  # In production, proper SASL/SSL authentication would be required
  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: podcast_kafka
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: true
    volumes:
      - kafka_data:/var/lib/kafka/data
    healthcheck:
      test: ["CMD", "kafka-topics", "--bootstrap-server", "localhost:9092", "--list"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Go Backend
  backend:
    build:
      context: ${BACKEND_PATH}
      dockerfile: Dockerfile
    container_name: podcast_backend
    environment:
      DATABASE_URL: ${DATABASE_URL}
      KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
      STORAGE_PATH: ${STORAGE_PATH}
      LOG_LEVEL: ${LOG_LEVEL}
      CORS_ORIGINS: ${CORS_ORIGINS}
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY}
      SERPER_API_KEY: ${SERPER_API_KEY}
      SERVER_PORT: ${SERVER_PORT}
    volumes:
      - transcript_storage:${STORAGE_PATH}
      # Development volume for hot reloading (remove for production)
      - ${BACKEND_PATH}:/app
    ports:
      - "${SERVER_PORT}:${SERVER_PORT}"
    depends_on:
      postgres:
        condition: service_healthy
      kafka:
        condition: service_healthy

  # Kafka Worker (same backend image, different entrypoint)
  kafka-worker:
    build:
      context: ${BACKEND_PATH}
      dockerfile: Dockerfile
    container_name: podcast_kafka_worker
    command: ["./worker"]
    environment:
      DATABASE_URL: ${DATABASE_URL}
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY}
      SERPER_API_KEY: ${SERPER_API_KEY}
      KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
      STORAGE_PATH: ${STORAGE_PATH}
      LOG_LEVEL: ${LOG_LEVEL}
    volumes:
      - transcript_storage:${STORAGE_PATH}
      # Development volume for hot reloading (remove for production)
      - ${BACKEND_PATH}:/app
    depends_on:
      postgres:
        condition: service_healthy
      kafka:
        condition: service_healthy

  # React Frontend
  frontend:
    build:
      context: ${FRONTEND_PATH}
      dockerfile: Dockerfile
    container_name: podcast_frontend
    environment:
      REACT_APP_API_BASE_URL: ${REACT_APP_API_BASE_URL}
    volumes:
      # Development volume for hot reloading (remove for production)
      - ${FRONTEND_PATH}/src:/app/src
      - ${FRONTEND_PATH}/public:/app/public
    ports:
      - "3000:3000"
    depends_on:
      - backend


volumes:
  postgres_data:
  zk_data:
  kafka_data:
  # Named volume for transcript storage (simulates S3)
  transcript_storage:
